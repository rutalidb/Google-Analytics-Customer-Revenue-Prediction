{
  "cells": [
    {
      "metadata": {
        "_uuid": "26ba695e833e3cf8e2cc05579f85b93ff380b9e8"
      },
      "cell_type": "markdown",
      "source": "        Data Fields\n\n**[] fullVisitorId**- A unique identifier for each user of the Google Merchandise Store.\n\n**[x] channelGrouping** - The channel via which the user came to the Store. \n\n**[x] date** - The date on which the user visited the Store.\n\n**[x] device** - The specifications for the device used to access the Store.\n\n**[x] geoNetwork** - This section contains information about the geography of the user.\n\n**[] sessionId** - A unique identifier for this visit to the store.\n\n**[x] socialEngagementType** - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n\n**[x] totals** - This section contains aggregate values across the session.\n\n**[x] trafficSource** - This section contains information about the Traffic Source from which the session originated.\n\n**[] visitId** - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.\n\n**[] visitNumber** - The session number for this user. If this is the first session, then this is set to 1.\n\n**[] visitStartTime** - The timestamp (expressed as POSIX time)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b9d046fbb5577d119f0b4748b396f23bd205d40",
        "_kg_hide-input": false
      },
      "cell_type": "code",
      "source": "# Import libraries\nimport pandas as pd\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cat\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nfrom sklearn.model_selection import train_test_split",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1063a287375c6fed2aa7a7cc974e42ce32510e07"
      },
      "cell_type": "markdown",
      "source": "**Function to read data from JSON format**"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def load_df(csv_path='../input/train-v1/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\n\nprint(os.listdir(\"../input/train-v1\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\ntrain_origin = load_df()\ntest_origin = load_df(\"../input/train-v1/test.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c59cf2ace1715444c51f4d6b554c2fb89b4940cc"
      },
      "cell_type": "code",
      "source": "train_origin.shape, test_origin.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d0abe3d9da9194efd4a66b5034bcab8aa879fd5d"
      },
      "cell_type": "code",
      "source": "train_origin.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd34562543a04dc936a4f0a1db35f6a9b75070c3"
      },
      "cell_type": "markdown",
      "source": "**Analyzing target**\n\nThe target we want to predict, transactionRevenue, is contained in one of the JSON columns, ie. the totals column. While loading the dataset, it was renamed as totals.transactionRevenue. The target only contains a few non-null values and before taking its log, we fill the NAs:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86a849f6b613897a84fe9673dfc8b83a587d13c5"
      },
      "cell_type": "code",
      "source": "target = train_origin['totals.transactionRevenue'].fillna(0).astype(float)\nprint(np.mean(target))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d537d912055c0a67020cbabbc88d86b734f59a9c"
      },
      "cell_type": "code",
      "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nsns.set(color_codes=True)\nsns.distplot(target[target > 0], kde=False, rug=True);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c7078b8567c2ac0b12a225a85f18876904653237"
      },
      "cell_type": "markdown",
      "source": "**Fill Nan Value for all other attributes**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a18ae7bfa368ca19bafd43c4008b5747d8932a4"
      },
      "cell_type": "code",
      "source": "columns_to_remove = [col for col in train_origin.columns if train_origin[col].nunique() == 1]\nprint(\"Nb. of variables with unique value: {}\".format(len(columns_to_remove)))\n\nfor col in columns_to_remove:\n    if set(['not available in demo dataset']) ==  set(train_origin[col].unique()): continue\n    print(col, train_origin[col].dtypes, train_origin[col].unique())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e5e5c6b3b8da8aae124b2aaf93748e4adf82513"
      },
      "cell_type": "code",
      "source": "# Do not remove columns with unique value and NAN, update value for NAN\ntrain_origin['totals.bounces'] = train_origin['totals.bounces'].fillna('0')\ntest_origin['totals.bounces'] = test_origin['totals.bounces'].fillna('0')\n\ntrain_origin['totals.newVisits'] = train_origin['totals.newVisits'].fillna('0')\ntest_origin['totals.newVisits'] = test_origin['totals.newVisits'].fillna('0')\n\ntrain_origin['trafficSource.adwordsClickInfo.isVideoAd'] = train_origin['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True)\ntest_origin['trafficSource.adwordsClickInfo.isVideoAd'] = test_origin['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True)\n\ntrain_origin['trafficSource.isTrueDirect'] = train_origin['trafficSource.isTrueDirect'].fillna(False)\ntest_origin['trafficSource.isTrueDirect'] = test_origin['trafficSource.isTrueDirect'].fillna(False)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7816b7aae236278c60781b14350ef210df523607"
      },
      "cell_type": "markdown",
      "source": "**Clear rare categories**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4665eacc2e92ee92bd64cc57623f8fe356ebd104"
      },
      "cell_type": "code",
      "source": "# Remove column not in test but in train, should we?\nprint(\"Columns in Train but NOT in Test:\")\nfor col in train_origin.columns:\n    if col not in test_origin.columns:\n        print(col)\nprint(\"Columns in Test but NOT in Traing:\")\nfor col in test_origin.columns:\n    if col not in train_origin.columns:\n        print(col)\ntrain_origin['trafficSource.campaignCode'].value_counts()\n# trafficSource.campaignCode is redundant, we will not use for training/testing",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9627975bc95a0aacb57eee3d27aeb8773e85319d"
      },
      "cell_type": "code",
      "source": "def clearRare(columnname, limit = 100):\n    # you may search for rare categories in train, train&test, or just test\n    #vc = pd.concat([train[columnname], test[columnname]], sort=False).value_counts()\n    vc = test_origin[columnname].value_counts()\n    \n    common = vc > limit\n    common = set(common.index[common].values)\n    print(\"Set\", sum(vc <= limit), columnname, \"categories to 'other';\", end=\" \")\n\n    train_origin.loc[train_origin[columnname].map(lambda x: x not in common), columnname] = 'other'\n    test_origin.loc[test_origin[columnname].map(lambda x: x not in common), columnname] = 'other'\n    print(\"now there are\", train_origin[columnname].nunique(), \"categories in train\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dc8f0062468e6a6b283bd2f88518685db82ffbc2"
      },
      "cell_type": "markdown",
      "source": "**Fill NAN exhausted**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5d76227e40fbe83f70c1b4269bb9b6b1fd481033"
      },
      "cell_type": "code",
      "source": "train_origin.fillna(0, inplace=True)\ntest_origin.fillna(0, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9babc7b5457116c39d2b384f892b238b29e7873"
      },
      "cell_type": "code",
      "source": "clearRare(\"device.browser\")\nclearRare(\"device.operatingSystem\")\nclearRare(\"geoNetwork.country\")\nclearRare(\"geoNetwork.city\")\nclearRare(\"geoNetwork.metro\")\nclearRare(\"geoNetwork.networkDomain\")\nclearRare(\"geoNetwork.region\")\nclearRare(\"geoNetwork.subContinent\")\nclearRare(\"trafficSource.adContent\")\nclearRare(\"trafficSource.campaign\")\nclearRare(\"trafficSource.keyword\")\nclearRare(\"trafficSource.medium\")\nclearRare(\"trafficSource.referralPath\")\nclearRare(\"trafficSource.source\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "125c64fe116dd97468fe5d28588d678cba9d5aa3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_origin.shape, train_origin.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f723211f5c5b00687155d55c3c4986fc3cbcc7f8"
      },
      "cell_type": "markdown",
      "source": "**Convert Date into Understandable format**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb8ed2577e40e8e361650e7d4080d39860001388"
      },
      "cell_type": "code",
      "source": "for df in [train_origin, test_origin]:\n    df_date = pd.to_datetime(df['visitStartTime'], unit='s')\n    df['sess_date_dow'] = df_date.dt.dayofweek\n    df['sess_date_hours'] = df_date.dt.hour\n    df['sess_date_dom'] = df_date.dt.day\ntrain_origin.shape, test_origin.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "56ccb7befcd245a95d17635c9573a567026b4224"
      },
      "cell_type": "markdown",
      "source": "**Analyzing transaction time of dataset**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c17e7786c83cfd2a39ecb43d0f5e215f9631601f"
      },
      "cell_type": "code",
      "source": "train_date = pd.to_datetime(train_origin.date, format=\"%Y%m%d\")\ntest_date = pd.to_datetime(test_origin.date, format=\"%Y%m%d\")\ntrain_date.value_counts().sort_index().plot(label=\"train\")\ntest_date.value_counts().sort_index().plot(label=\"test\")\n# plt.legend()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b08b03344dedf12bc76343cecfa6610ef46eb66"
      },
      "cell_type": "code",
      "source": "# distribution by day of week\nsns.set(color_codes=True)\nsns.distplot(train_origin['sess_date_dow'], kde=False, rug=True);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a080460612e5e1dffefd74cba397881d2cd8dc51"
      },
      "cell_type": "code",
      "source": "# Show date of week when people have made transactions\ndateWeek = train_origin[train_origin['totals.transactionRevenue'] != 0]\nsns.distplot(dateWeek['sess_date_dow'], kde=False, rug=True);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bd21c9f5a0e70a203d454d94f84234c24e2f78a5"
      },
      "cell_type": "code",
      "source": "# distribution by hour of day\nsns.distplot(train_origin['sess_date_hours'], kde=False, rug=True);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ceb6de3a1b68f8b56a8202d04f546d347b87dfc9"
      },
      "cell_type": "code",
      "source": "# Show hours when people have made transactions\nhourDate = train_origin[train_origin['totals.transactionRevenue'] != 0]\nsns.distplot(hourDate['sess_date_hours'], kde=False, rug=True);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "650d98cfb90d68ddcaed9e0768588ccbd6dfc180"
      },
      "cell_type": "code",
      "source": "# distribution by day of month\nsns.distplot(train_origin['sess_date_dom'], kde=False, rug=True);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1653abf1f2851a129af02ff3f69e064b3a88b635"
      },
      "cell_type": "code",
      "source": "# Show date of month when people have done transactions\ndateMonth = train_origin[train_origin['totals.transactionRevenue'] != 0]\nsns.distplot(dateMonth['sess_date_dom'], kde=False, rug=True);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "120c5af8df5e3b1ae1a15b0b27ca37a5c9101979"
      },
      "cell_type": "code",
      "source": "for col in train_origin.columns:\n    print(col, train_origin[col].dtype)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d9890edca08d0947dd79a4ef405631902587f6f"
      },
      "cell_type": "markdown",
      "source": "**Create feature list**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f173f473afcfef8b703564bb91dc851482cbdfb"
      },
      "cell_type": "code",
      "source": "excluded_features = [\n    'date', 'sessionId', 'fullVisitorId', \n    'totals.transactionRevenue', 'trafficSource.campaignCode',\n    'visitId', 'visitStartTime','visitNumber'\n]\n\nno_object_columns = ['device.isMobile', 'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.isTrueDirect',\n                    'sess_date_dow', 'sess_date_hours', 'sess_date_dom']\n\ncategorical_features = [\n    _f for _f in train_origin.columns\n    if (_f not in excluded_features) # & (train_df[_f].dtype == 'object')\n]\nprint(\"Number of selected features : \",len(categorical_features))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9b31ddb180191f354fb3d488a3925856c801ce9d"
      },
      "cell_type": "markdown",
      "source": "**Final columns after filter columns with unique values**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee66b849cfe18a8c931c2a1f50e589730e1482ec"
      },
      "cell_type": "code",
      "source": "columns = [col for col in train_origin.columns if  train_origin[col].nunique() > 1 and (col in categorical_features)]\nprint(len(columns))\nprint(columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "231948e3e50f04612269be64e6b9e976c632b3b5"
      },
      "cell_type": "markdown",
      "source": "**DONE for select feature and factorizing dataset**"
    },
    {
      "metadata": {
        "_uuid": "9a1f5932b5a6ddeec7c4227d416e7eebe95a75be"
      },
      "cell_type": "markdown",
      "source": "**DATA Visualization**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1be8954461d01c283151e67cd36cbb2a1180f8ad"
      },
      "cell_type": "code",
      "source": "train_origin['geoNetwork.continent'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec406601313c11da06b4c954773bbe165e3557f5"
      },
      "cell_type": "code",
      "source": "# Analysis total revenue by continent\nttt = train_origin[train_origin['totals.transactionRevenue'] != 0].copy()\nttt['totals.transactionRevenue'] = ttt['totals.transactionRevenue'].astype(float)\nttt.groupby(by='geoNetwork.continent')['totals.transactionRevenue'].sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "90113329d879a314b2017375add92a81079c96f4",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def drawBars(columnname):\n    sns.barplot(x=\"count\", y=\"index\", hue=\"dataset\",\n        data=pd.melt(pd.concat([train_origin[columnname].value_counts().rename(\"train\"), \n                       test_origin[columnname].value_counts().rename(\"test\")], axis=1, sort=\"False\").reset_index(),\n            id_vars=\"index\", var_name=\"dataset\", value_name=\"count\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6fb8595b779a2fe75330ed7697fecf7bda26a7bd"
      },
      "cell_type": "code",
      "source": "#find country where total_transactionRevenue > 0\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "64ec061fed856205cfb544fc955e13d43fd01c0d"
      },
      "cell_type": "code",
      "source": "def plot_countryMap(data, country, nvisits, visits, title, colormap='Viridis'):\n    data = dict(type = 'choropleth', \n                colorscale = colormap,\n                autocolorscale = False,\n                reversescale = False,\n               locations = data[country],\n               locationmode = 'country names',\n               z = data[nvisits], \n               text = data[nvisits],\n               colorbar = {'title':visits})\n    layout = dict(title = title, \n                 geo = dict(showframe = False, \n                         projection = {'type': 'natural earth'}))\n    choromap = go.Figure(data = [data], layout=layout)\n    iplot(choromap)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7f47361465a254420524a5e6765cac1726b7190f"
      },
      "cell_type": "code",
      "source": "#visits per country\ncountries = train_origin['geoNetwork.country'].value_counts()\ncountryVisits = pd.DataFrame(data={'geoNetwork_country': countries.values}, index=countries.index).reset_index()\ncountryVisits.columns = ['Country', 'Visits']\nplot_countryMap(countryVisits, 'Country', 'Visits', 'Visits', 'Visits per country')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "48fc2ebee72bab5a01eeee4133b2946f8eeda9ed"
      },
      "cell_type": "code",
      "source": "drawBars(\"channelGrouping\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dfcd647f58b830f4e99e8dca94b13733c1849e73"
      },
      "cell_type": "code",
      "source": "#visits with non-zero transaction\nnonZero = train_origin[train_origin['totals.transactionRevenue'] != 0].copy()\ncountries = nonZero['geoNetwork.country'].value_counts()\ncountryVisits = pd.DataFrame(data={'geoNetwork_country': countries.values}, index=countries.index).reset_index()\ncountryVisits.columns = ['Country', 'Visits']\nplot_countryMap(countryVisits, 'Country', 'Visits', 'Visits', 'Visits with non-zero transaction per country')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b4a9f60d9a03cc949be2628ab059411e6583afb"
      },
      "cell_type": "code",
      "source": "ids_train = set(train_origin.fullVisitorId.unique())\nids_test = set(test_origin.fullVisitorId.unique())\nprint(\"Unique visitor ids in train:\", len(ids_train))\nprint(\"Unique visitor ids in test:\", len(ids_test))\nprint(\"Common visitors in train and test:\", len(ids_train & ids_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38d6484bf724c28e23b303fde88550307dbf78f4"
      },
      "cell_type": "code",
      "source": "print(\"Train: \", np.bincount(train_origin.visitId.value_counts()))\nprint(\"test: \", np.bincount(test_origin.visitId.value_counts()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "84c018622dc9476c0c74b7694a50b634f753ec27"
      },
      "cell_type": "markdown",
      "source": "\n**Factorization**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "897271c4021c5e50261f9e3478506139efcd6e7d"
      },
      "cell_type": "code",
      "source": "object_columns = [i for i in columns if i not in no_object_columns]\nfor f in object_columns:\n    print(f)\n    train_origin[f], indexer = pd.factorize(train_origin[f])\n    test_origin[f] = indexer.get_indexer(test_origin[f])\ntrain_origin[\"totals.transactionRevenue\"] = train_origin[\"totals.transactionRevenue\"].fillna(0).astype(int)\ntrain_origin[\"totals.transactionRevenue\"] = train_origin[\"totals.transactionRevenue\"].astype('int')\ntrain_origin.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "37d9cd291d49b44cd1e22fe59f81758e317ae352"
      },
      "cell_type": "markdown",
      "source": "**Build Models**"
    },
    {
      "metadata": {
        "_uuid": "4610501c0e7787b75bb2705db69222f2dd41262e"
      },
      "cell_type": "markdown",
      "source": "**Classify non-zero revenues**\n\nThere are only 1.3% of the sessions have a non-zero revenue. Thus, we first CLASSIFY session, and then using regression on non-zero set."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e6b4172679d2e7340b00c16f70e273f334050de0"
      },
      "cell_type": "code",
      "source": "train_df = train_origin.copy()\ntest_df = test_origin.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "70829881b0cd0e6499747b009807c4af661cb06d"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nx = train_df[columns]\ny = train_df[\"totals.transactionRevenue\"]\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=42)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5674f27d383837f719d2bcf01696b6ad178e5fc7"
      },
      "cell_type": "code",
      "source": "y_valid[y_valid > 0].shape, y_train[y_train > 0].shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef4eb0d2ab81e9ce86ada18406e710310b04eb71"
      },
      "cell_type": "code",
      "source": "# Transfer target into format of [0, 1]\ny_train01 = y_train.copy()\ny_train01[y_train01 > 0] = 1\n\ny_valid01 = y_valid.copy()\ny_valid01[y_valid01 > 0] = 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1f5d1ce5283acdf8f26316d4f9cdd16b79a708d"
      },
      "cell_type": "code",
      "source": "# Ratio of classification 0, 1\ny_train01.value_counts() / y_train01.shape[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "62245a8fa3cdb81be4fd0bcbdd0d4218ee5fae27"
      },
      "cell_type": "markdown",
      "source": "**Visualize data distribution using PCA**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46a92ad7eaf00f570d03daeb75bd7686f25812d1"
      },
      "cell_type": "code",
      "source": "from sklearn.decomposition import PCA\npca = PCA(n_components = 2).fit(x)\nX_reduced = pca.transform(x)\n\nyy = y.copy()\nyy[yy>0] = 1\n\nplt.clf()\nfig, ax = plt.subplots()\n\n# Plot the training points\nlabels = ['0','1']\ncolors = ['white','red']\nfor i in range(2):   \n    ax.scatter(X_reduced[yy==i, 0], X_reduced[yy==i, 1], c=colors[i],label = labels[i],cmap=plt.cm.Set1,s=40,\n                edgecolor=None,alpha=0.3)\n\n\nplt.title(\"First two PCA directions\")\nplt.xlabel(\"1st PC\")\nplt.ylabel(\"2nd PC\")\nax.legend()\nplt.xticks(())\nplt.yticks(())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0f4c71e8e5f13f5f01cfe2863006aadbb1d919ba"
      },
      "cell_type": "markdown",
      "source": "**XGBoost Classifier**\n\n*Paramters tuning:* learning rate,\nn estimators,\nmin child weight,\nmax delta step, booster,\nseed"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad5a5f2da1e5ab265a1ca3d5d6c94b81baca6efd"
      },
      "cell_type": "code",
      "source": "from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nxgb = XGBClassifier()\nxgb_param = {'n_estimators':[100, 500, 1000]}\n\nclf = GridSearchCV(xgb, xgb_param, cv=5, verbose=True, scoring='f1')\nclf.fit(x_train,y_train01)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80dd4b9094a629acd85880fb120e09c39ccd53f9"
      },
      "cell_type": "code",
      "source": "clf.best_estimator_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d691de7cc08e3a84f34966a16d4ed2e11db9b47"
      },
      "cell_type": "code",
      "source": "y_pred = clf.predict(x_train)\ny_pred_valid = clf.predict(x_valid)\nprint(\"f1 score:\")\nprint(\"Train data: \" + str(f1_score(y_train01, y_pred)))\nprint(\"Test data: \" + str(f1_score(y_valid01, y_pred_valid)))\n\nprint()\nprint(\"Accuracy Score:\")\nprint(\"Train data: \" + str(accuracy_score(y_train01, y_pred)))\nprint(\"Test data: \" + str(accuracy_score(y_valid01, y_pred_valid)))\nprint()\n\nprint(\"ROC AUC Score:\")\nprint(\"Train data: \" + str(roc_auc_score(y_train01, y_pred)))\nprint(\"Test data: \" + str(roc_auc_score(y_valid01, y_pred_valid)))\nprint()\n\n#Confusion Matrix\nprint(\"Confusion Matrix\")\ny_actual = pd.Series(y_valid01, name='Actual')\ny_predict = pd.Series(y_pred_valid, name='Predicted')\ndf_confusion = pd.crosstab(y_actual, y_predict)\nprint(df_confusion)\n\nplt.figure(figsize=(15, 15), dpi=80, facecolor='w', edgecolor='k')\nplt.matshow(df_confusion, cmap='viridis')\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(df_confusion.columns))\nplt.xticks(tick_marks, df_confusion.columns, rotation=45)\nplt.yticks(tick_marks, df_confusion.index)\nplt.ylabel(df_confusion.index.name)\nplt.xlabel(df_confusion.columns.name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db19bf7d26617fcf200af7ec65661e0a242fa100"
      },
      "cell_type": "code",
      "source": "# Predict test\ny_submit_xgb = clf.predict(test_df[columns])\ny_submit_xgb[y_submit_xgb > 0] = 10272.0\npd.Series(y_submit_xgb).value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "67754262dfaf38cd3d189e9b2a3e4acce6ff09ac"
      },
      "cell_type": "code",
      "source": "# Prediction\ny_submit = clf.predict(test_df[columns])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "929faae90d58d7cc53620215c07431d609c6b73c"
      },
      "cell_type": "code",
      "source": "#Predicted revenue per customer\nsubmission = test_origin[['fullVisitorId']].copy()\nsubmission.loc[:, 'PredictedLogRevenue'] = y_submit\ngrouped_test_RF = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\ngrouped_test_RF[\"PredictedLogRevenue\"] = np.log1p(grouped_test_RF[\"PredictedLogRevenue\"])\nprint(grouped_test_RF)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e19b447007732957974708b3139e4046f13af701"
      },
      "cell_type": "markdown",
      "source": "**Random Forest**\n\n*Tuning paramters*\nn_estimators,\ncriterion,\nmax_depth,\nmax_features,\nmin_samples_split,\nmin_samples_leaf"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e890baf57ab2ad68a29bac4c0d5529cb2543d7c6"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV\nrf = RandomForestClassifier()\nrf_param = {'n_estimators':[100, 500, 1000]}\n\nclf_rf = GridSearchCV(rf,rf_param,cv=5,verbose=True,scoring='f1')\nclf_rf.fit(x_train,y_train01)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dcfca29415001fff4b2869f1a93867052883800a"
      },
      "cell_type": "code",
      "source": "clf_rf.best_estimator_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dab8f652a9ef436273df6aee70e907935ba7e889"
      },
      "cell_type": "code",
      "source": "y_pred = clf_rf.predict(x_train)\ny_pred_valid = clf_rf.predict(x_valid)\n\nprint(\"f1 score:\")\nprint(\"Train data: \" + str(f1_score(y_train01, y_pred)))\nprint(\"Test data: \" + str(f1_score(y_valid01, y_pred_valid)))\n\nprint()\nprint(\"Accuracy Score:\")\nprint(\"Train data: \" + str(accuracy_score(y_train01, y_pred)))\nprint(\"Test data: \" + str(accuracy_score(y_valid01, y_pred_valid)))\nprint()\n\nprint(\"ROC AUC Score:\")\nprint(\"Train data: \" + str(roc_auc_score(y_train01, y_pred)))\nprint(\"Test data: \" + str(roc_auc_score(y_valid01, y_pred_valid)))\nprint()\n\n#Confusion Matrix\nprint(\"Confusion Matrix\")\ny_actual = pd.Series(y_valid01, name='Actual')\ny_predict = pd.Series(y_pred_valid, name='Predicted')\ndf_confusion = pd.crosstab(y_actual, y_predict)\nprint(df_confusion)\n\nplt.figure(figsize=(15, 15), dpi=80, facecolor='w', edgecolor='k')\nplt.matshow(df_confusion, cmap='viridis')\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(df_confusion.columns))\nplt.xticks(tick_marks, df_confusion.columns, rotation=45)\nplt.yticks(tick_marks, df_confusion.index)\nplt.ylabel(df_confusion.index.name)\nplt.xlabel(df_confusion.columns.name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aeb4a0a37a4ff4c42aa7ab86bade10d4e6ef8258"
      },
      "cell_type": "code",
      "source": "# Predict test\ny_submit_rf = clf_rf.predict(test_df[columns])\ny_submit_rf[y_submit_rf > 0] = 10272.0\npd.Series(y_submit_rf).value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "53370ccceb51ddc24caff6b08e9696071a8fa3ac"
      },
      "cell_type": "code",
      "source": "# Prediction\ny_submit = clf_rf.predict(test_df[columns])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7353b46be828643c9573ced1e6356e445696086d"
      },
      "cell_type": "code",
      "source": "#Predicted Revenue per customer output\nsubmission = test_origin[['fullVisitorId']].copy()\nsubmission.loc[:, 'PredictedLogRevenue'] = y_submit\ngrouped_test_RF = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\ngrouped_test_RF[\"PredictedLogRevenue\"] = np.log1p(grouped_test_RF[\"PredictedLogRevenue\"])\nprint(grouped_test_RF)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ec7a2637ac8ba6437238367b9497f09ec6093155"
      },
      "cell_type": "markdown",
      "source": "**Ensemble learning of classifiers RF + XGB**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "403f38643e3e4ed21130702dc365ff5954c6ea8b"
      },
      "cell_type": "code",
      "source": "y_combination = (y_submit_rf + y_submit_xgb) / 2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0bdabe9dd12d95dfcded2466783500b209b8bb4b"
      },
      "cell_type": "code",
      "source": "print(\"Confusion Matrix\")\ny_actual = pd.Series(y_valid01, name='Actual')\ny_predict = pd.Series(y_combination, name='Predicted')\ndf_confusion = pd.crosstab(y_actual, y_predict)\nprint(df_confusion)\n\nplt.figure(figsize=(15, 15), dpi=80, facecolor='w', edgecolor='k')\nplt.matshow(df_confusion, cmap='viridis')\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(df_confusion.columns))\nplt.xticks(tick_marks, df_confusion.columns, rotation=45)\nplt.yticks(tick_marks, df_confusion.index)\nplt.ylabel(df_confusion.index.name)\nplt.xlabel(df_confusion.columns.name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d704ff797f5fc034dd9c03bd5d517caaec41651"
      },
      "cell_type": "code",
      "source": "#Predicted revenue per customer\nsubmission = test_origin[['fullVisitorId']].copy()\nsubmission.loc[:, 'PredictedLogRevenue'] = y_combination\ngrouped_test_RF = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\ngrouped_test_RF[\"PredictedLogRevenue\"] = np.log1p(grouped_test_RF[\"PredictedLogRevenue\"])\ngrouped_test_RF.to_csv('submit_Clf.csv',index=False)\nprint(grouped_test_RF)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ee561b24dd5fe93f99710cba7e935ea4793bd4fa"
      },
      "cell_type": "markdown",
      "source": "**Predict revenues for non-zero**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "249689bb7ac12cdf77043d05fb22cc76e7cdf35d"
      },
      "cell_type": "code",
      "source": "train_origin[columns].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11a5f8048f1dd9aebbdc118574e28c5069e676c4"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\n\ndef score(data, y):\n    validation_res = pd.DataFrame(\n    {\"fullVisitorId\": data[\"fullVisitorId\"].values,\n     \"transactionRevenue\": data[\"totals.transactionRevenue\"].values,\n     \"predictedRevenue\": np.expm1(y)})\n\n    validation_res = validation_res.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"predictedRevenue\"].sum().reset_index()\n    return np.sqrt(mean_squared_error(np.log1p(validation_res[\"transactionRevenue\"].values), \n                                     np.log1p(validation_res[\"predictedRevenue\"].values)))\n\nclass KFoldValidation():\n    def __init__(self, data, n_splits=5):\n        unique_vis = np.array(sorted(data['fullVisitorId'].astype(str).unique()))\n        folds = GroupKFold(n_splits)\n        ids = np.arange(data.shape[0])\n        \n        self.fold_ids = []\n        for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n            self.fold_ids.append([\n                    ids[data['fullVisitorId'].astype(str).isin(unique_vis[trn_vis])],\n                    ids[data['fullVisitorId'].astype(str).isin(unique_vis[val_vis])]\n                ])\n            \n    def validate(self, train, test, features, model, name=\"\", prepare_stacking=False, \n                 fit_params={\"early_stopping_rounds\": 50, \"verbose\": 100, \"eval_metric\": \"rmse\"}):\n        model.FI = pd.DataFrame(index=features)\n        full_score = 0\n        \n        if prepare_stacking:\n            test[name] = 0\n            train[name] = np.NaN\n        \n        for fold_id, (trn, val) in enumerate(self.fold_ids):\n            devel = train[features].iloc[trn]\n            y_devel = np.log1p(train[\"totals.transactionRevenue\"].iloc[trn])\n            valid = train[features].iloc[val]\n            y_valid = np.log1p(train[\"totals.transactionRevenue\"].iloc[val])\n                       \n            print(\"Fold \", fold_id, \":\")\n            model.fit(devel, y_devel, eval_set=[(valid, y_valid)], **fit_params)\n            \n            if len(model.feature_importances_) == len(features):  # some bugs in catboost?\n                model.FI['fold' + str(fold_id)] = model.feature_importances_ / model.feature_importances_.sum()\n\n            predictions = model.predict(valid)\n            predictions[predictions < 0] = 0\n            print(\"Fold \", fold_id, \" error: \", mean_squared_error(y_valid, predictions)**0.5)\n            \n            fold_score = score(train.iloc[val], predictions)\n            full_score += fold_score / len(self.fold_ids)\n            print(\"Fold \", fold_id, \" score: \", fold_score)\n            \n            if prepare_stacking:\n                train[name].iloc[val] = predictions\n                \n                test_predictions = model.predict(test[features])\n                test_predictions[test_predictions < 0] = 0\n                test[name] += test_predictions / len(self.fold_ids)\n        print(\"Confusion Matrix\")\n        y_actual = pd.Series(y_valid, name='Actual')\n        y_predict = pd.Series(predictions, name='Predicted')\n        df_confusion = pd.crosstab(y_actual, y_predict)\n        print(df_confusion)\n\n        plt.figure(figsize=(15, 15), dpi=80, facecolor='w', edgecolor='k')\n        plt.matshow(df_confusion, cmap='viridis')\n        plt.title('Confusion matrix')\n        plt.colorbar()\n        tick_marks = np.arange(len(df_confusion.columns))\n        plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n        plt.yticks(tick_marks, df_confusion.index)\n        plt.ylabel(df_confusion.index.name)\n        plt.xlabel(df_confusion.columns.name)        \n        print(\"Final score: \", full_score)\n        return full_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2df239dd6b4fc904a983fef6bfd6481bcd7b7021"
      },
      "cell_type": "code",
      "source": "Kfolder = KFoldValidation(train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f94f2c779f16b98750b66d78b047f7de1a0c5128"
      },
      "cell_type": "markdown",
      "source": "**LGBMRegressor**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "490968445405b86c144df1e2514374dcc5c6af5d"
      },
      "cell_type": "code",
      "source": "lgbmodel = lgb.LGBMRegressor(n_estimators=1000, objective=\"regression\", metric=\"rmse\", num_leaves=31, min_child_samples=100,\n                      learning_rate=0.03, bagging_fraction=0.7, feature_fraction=0.5, bagging_frequency=5, \n                      bagging_seed=2019, subsample=.9, colsample_bytree=.9, use_best_model=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "817a1bbf0812db8da690dba3d553055d2e7fcba8"
      },
      "cell_type": "code",
      "source": "Kfolder.validate(train_df, test_df, columns, lgbmodel, \"lgbpred\", prepare_stacking=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8761f57a1fbc65f0dc3b2f8d57b960db5c97cad9"
      },
      "cell_type": "code",
      "source": "# Prediction\ny_submit = lgbmodel.predict(test_df[columns])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a878a29b8517ed1919dc906813f2f85ac5a06f6c"
      },
      "cell_type": "code",
      "source": "# output to file\nsubmission = test_origin[['fullVisitorId']].copy()\nsubmission.loc[:, 'PredictedLogRevenue'] = y_submit\ngrouped_test_LGB = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\ngrouped_test_LGB[\"PredictedLogRevenue\"] = np.log1p(grouped_test_LGB[\"PredictedLogRevenue\"])\ngrouped_test_LGB[\"PredictedLogRevenue\"] = grouped_test_LGB[\"PredictedLogRevenue\"].fillna(0).astype(float)\ngrouped_test_LGB.to_csv('submit_reg_lgb.csv',index=False)\ntest_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "46dc11c84f46283e62e7d7f894137c50dae572ab"
      },
      "cell_type": "markdown",
      "source": "**Cat Model**\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a09aae227ca9bab2e9e1372f1c1daaf13b3f54a"
      },
      "cell_type": "code",
      "source": "catmodel = cat.CatBoostRegressor(iterations=1000, learning_rate=0.2, depth=5, random_seed=2019)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4edc701c4eda9cafca61a49a0f2f48eac7d89ce4"
      },
      "cell_type": "code",
      "source": "Kfolder.validate(train_df, test_df, columns, catmodel, name=\"catpred\", prepare_stacking=True,\n                fit_params={\"use_best_model\": True, \"verbose\": 100})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ddb9159c477831896734836c72c7d81339931c4"
      },
      "cell_type": "code",
      "source": "# Prediction\ny_submit = catmodel.predict(test_df[columns])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22296d3d693c13a6446ccca2554e954fc14c4cbe"
      },
      "cell_type": "code",
      "source": "# output to file\nsubmission = test_origin[['fullVisitorId']].copy()\nsubmission.loc[:, 'PredictedLogRevenue'] = y_submit\ngrouped_test_CAT = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\ngrouped_test_CAT[\"PredictedLogRevenue\"] = np.log1p(grouped_test_CAT[\"PredictedLogRevenue\"])\ngrouped_test_CAT[\"PredictedLogRevenue\"] = grouped_test_CAT[\"PredictedLogRevenue\"].fillna(0).astype(float)\ngrouped_test_CAT.to_csv('submit_reg_cat.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "886de969c1e666d68f3e977599ce9cc8cf36a1a0"
      },
      "cell_type": "code",
      "source": "test_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d26d318da867d476ecd652dd35aea68cd228f867"
      },
      "cell_type": "markdown",
      "source": "**Ensemble learning **\n\nIn this part, we use result of models (LGB, CAT) for ensemble learning"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a688e83273744ad238266e368b7fc0a81b224d9c"
      },
      "cell_type": "code",
      "source": "train_df['PredictedLogRevenue'] = 0.6 * train_df[\"lgbpred\"] + 0.4 * train_df[\"catpred\"] \nscore(train_df, train_df.PredictedLogRevenue)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "12a665823885d8e7b59b5b13faba78ab19a4ebca"
      },
      "cell_type": "code",
      "source": "test_df['PredictedLogRevenue'] = 0.6 * test_df[\"lgbpred\"] +  0.4 * test_df[\"catpred\"]\ny_submit =  test_df['PredictedLogRevenue']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa02b08d82731ae3adfb482d7d52f2169d3fd74c"
      },
      "cell_type": "code",
      "source": "# output to file\nsubmission = test_origin[['fullVisitorId']].copy()\nsubmission.loc[:, 'PredictedLogRevenue'] = y_submit\ngrouped_test_ENS = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\ngrouped_test_ENS[\"PredictedLogRevenue\"] = np.log1p(grouped_test_ENS[\"PredictedLogRevenue\"])\ngrouped_test_ENS[\"PredictedLogRevenue\"] = grouped_test_ENS[\"PredictedLogRevenue\"].fillna(0).astype(float)\ngrouped_test_ENS.to_csv('submit_reg_ens.csv',index=False)\ngrouped_test_ENS.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68175c135a519977703e0c5d21009a1087f34a8b"
      },
      "cell_type": "markdown",
      "source": "**======== THE END ========**"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}